{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tutorial 5: Clustering & Anomaly Detection Using `scikit-learn` and `scipy`\n",
    "\n",
    "In this assignment you will explore the concepts of clustering and anomaly detection using Python's `scikit-learn` and `scipy` libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# some imports are provided here for you\n",
    "# you may add additional imports if needed\n",
    "import warnings\n",
    "warnings.simplefilter(\"ignore\")\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn as sk\n",
    "import matplotlib.pyplot as plt\n",
    "import random as rand\n",
    "import itertools as it\n",
    "\n",
    "from pandas import DataFrame, Series\n",
    "\n",
    "from scipy.cluster import hierarchy as hier\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage\n",
    "from scipy.spatial.distance import pdist\n",
    "\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import MDS\n",
    "from sklearn.cluster import KMeans, AgglomerativeClustering, DBSCAN\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.metrics import accuracy_score, silhouette_score\n",
    "from sklearn.model_selection import StratifiedKFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data\n",
    "\n",
    "For this assigment you'll be working with the Epileptic Seizure Recognition Data Set developed at Rochester Institute of Technology. In this data set each row is a sample of the EEG recording at a different point in time. The dataset contains 179 columns, the first 178 are the EEG features while the last column represents the label y {1, 2, 3, 4, 5}. All subjects falling in classes 2, 3, 4, and 5 are subjects who did not have epileptic seizure. Only subjects in class 1 have epileptic seizure. The class labels will not be used for clustering (which is an *unsupervised* task!) - they will just be used for plotting the data. The only change that was made to the original dataset was removing the first column, which contained the row indexes. You can find additional information about the dataset [here](https://archive.ics.uci.edu/ml/datasets/Epileptic+Seizure+Recognition)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(11500, 179)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>X1</th>\n",
       "      <th>X2</th>\n",
       "      <th>X3</th>\n",
       "      <th>X4</th>\n",
       "      <th>X5</th>\n",
       "      <th>X6</th>\n",
       "      <th>X7</th>\n",
       "      <th>X8</th>\n",
       "      <th>X9</th>\n",
       "      <th>X10</th>\n",
       "      <th>...</th>\n",
       "      <th>X170</th>\n",
       "      <th>X171</th>\n",
       "      <th>X172</th>\n",
       "      <th>X173</th>\n",
       "      <th>X174</th>\n",
       "      <th>X175</th>\n",
       "      <th>X176</th>\n",
       "      <th>X177</th>\n",
       "      <th>X178</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>135</td>\n",
       "      <td>190</td>\n",
       "      <td>229</td>\n",
       "      <td>223</td>\n",
       "      <td>192</td>\n",
       "      <td>125</td>\n",
       "      <td>55</td>\n",
       "      <td>-9</td>\n",
       "      <td>-33</td>\n",
       "      <td>-38</td>\n",
       "      <td>...</td>\n",
       "      <td>-17</td>\n",
       "      <td>-15</td>\n",
       "      <td>-31</td>\n",
       "      <td>-77</td>\n",
       "      <td>-103</td>\n",
       "      <td>-127</td>\n",
       "      <td>-116</td>\n",
       "      <td>-83</td>\n",
       "      <td>-51</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>386</td>\n",
       "      <td>382</td>\n",
       "      <td>356</td>\n",
       "      <td>331</td>\n",
       "      <td>320</td>\n",
       "      <td>315</td>\n",
       "      <td>307</td>\n",
       "      <td>272</td>\n",
       "      <td>244</td>\n",
       "      <td>232</td>\n",
       "      <td>...</td>\n",
       "      <td>164</td>\n",
       "      <td>150</td>\n",
       "      <td>146</td>\n",
       "      <td>152</td>\n",
       "      <td>157</td>\n",
       "      <td>156</td>\n",
       "      <td>154</td>\n",
       "      <td>143</td>\n",
       "      <td>129</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-32</td>\n",
       "      <td>-39</td>\n",
       "      <td>-47</td>\n",
       "      <td>-37</td>\n",
       "      <td>-32</td>\n",
       "      <td>-36</td>\n",
       "      <td>-57</td>\n",
       "      <td>-73</td>\n",
       "      <td>-85</td>\n",
       "      <td>-94</td>\n",
       "      <td>...</td>\n",
       "      <td>57</td>\n",
       "      <td>64</td>\n",
       "      <td>48</td>\n",
       "      <td>19</td>\n",
       "      <td>-12</td>\n",
       "      <td>-30</td>\n",
       "      <td>-35</td>\n",
       "      <td>-35</td>\n",
       "      <td>-36</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-105</td>\n",
       "      <td>-101</td>\n",
       "      <td>-96</td>\n",
       "      <td>-92</td>\n",
       "      <td>-89</td>\n",
       "      <td>-95</td>\n",
       "      <td>-102</td>\n",
       "      <td>-100</td>\n",
       "      <td>-87</td>\n",
       "      <td>-79</td>\n",
       "      <td>...</td>\n",
       "      <td>-82</td>\n",
       "      <td>-81</td>\n",
       "      <td>-80</td>\n",
       "      <td>-77</td>\n",
       "      <td>-85</td>\n",
       "      <td>-77</td>\n",
       "      <td>-72</td>\n",
       "      <td>-69</td>\n",
       "      <td>-65</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-9</td>\n",
       "      <td>-65</td>\n",
       "      <td>-98</td>\n",
       "      <td>-102</td>\n",
       "      <td>-78</td>\n",
       "      <td>-48</td>\n",
       "      <td>-16</td>\n",
       "      <td>0</td>\n",
       "      <td>-21</td>\n",
       "      <td>-59</td>\n",
       "      <td>...</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>-12</td>\n",
       "      <td>-32</td>\n",
       "      <td>-41</td>\n",
       "      <td>-65</td>\n",
       "      <td>-83</td>\n",
       "      <td>-89</td>\n",
       "      <td>-73</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>55</td>\n",
       "      <td>28</td>\n",
       "      <td>18</td>\n",
       "      <td>16</td>\n",
       "      <td>16</td>\n",
       "      <td>19</td>\n",
       "      <td>25</td>\n",
       "      <td>40</td>\n",
       "      <td>52</td>\n",
       "      <td>66</td>\n",
       "      <td>...</td>\n",
       "      <td>-12</td>\n",
       "      <td>-31</td>\n",
       "      <td>-42</td>\n",
       "      <td>-54</td>\n",
       "      <td>-60</td>\n",
       "      <td>-64</td>\n",
       "      <td>-60</td>\n",
       "      <td>-56</td>\n",
       "      <td>-55</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>-55</td>\n",
       "      <td>-9</td>\n",
       "      <td>52</td>\n",
       "      <td>111</td>\n",
       "      <td>135</td>\n",
       "      <td>129</td>\n",
       "      <td>103</td>\n",
       "      <td>72</td>\n",
       "      <td>37</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>-125</td>\n",
       "      <td>-99</td>\n",
       "      <td>-79</td>\n",
       "      <td>-62</td>\n",
       "      <td>-41</td>\n",
       "      <td>-26</td>\n",
       "      <td>11</td>\n",
       "      <td>67</td>\n",
       "      <td>128</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1</td>\n",
       "      <td>-2</td>\n",
       "      <td>-8</td>\n",
       "      <td>-11</td>\n",
       "      <td>-12</td>\n",
       "      <td>-17</td>\n",
       "      <td>-15</td>\n",
       "      <td>-16</td>\n",
       "      <td>-18</td>\n",
       "      <td>-17</td>\n",
       "      <td>...</td>\n",
       "      <td>-79</td>\n",
       "      <td>-91</td>\n",
       "      <td>-97</td>\n",
       "      <td>-88</td>\n",
       "      <td>-76</td>\n",
       "      <td>-72</td>\n",
       "      <td>-66</td>\n",
       "      <td>-57</td>\n",
       "      <td>-39</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>-278</td>\n",
       "      <td>-246</td>\n",
       "      <td>-215</td>\n",
       "      <td>-191</td>\n",
       "      <td>-177</td>\n",
       "      <td>-167</td>\n",
       "      <td>-157</td>\n",
       "      <td>-139</td>\n",
       "      <td>-118</td>\n",
       "      <td>-92</td>\n",
       "      <td>...</td>\n",
       "      <td>-400</td>\n",
       "      <td>-379</td>\n",
       "      <td>-336</td>\n",
       "      <td>-281</td>\n",
       "      <td>-226</td>\n",
       "      <td>-174</td>\n",
       "      <td>-125</td>\n",
       "      <td>-79</td>\n",
       "      <td>-40</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>8</td>\n",
       "      <td>15</td>\n",
       "      <td>13</td>\n",
       "      <td>3</td>\n",
       "      <td>-6</td>\n",
       "      <td>-8</td>\n",
       "      <td>-5</td>\n",
       "      <td>4</td>\n",
       "      <td>25</td>\n",
       "      <td>41</td>\n",
       "      <td>...</td>\n",
       "      <td>49</td>\n",
       "      <td>31</td>\n",
       "      <td>11</td>\n",
       "      <td>-5</td>\n",
       "      <td>-17</td>\n",
       "      <td>-19</td>\n",
       "      <td>-15</td>\n",
       "      <td>-15</td>\n",
       "      <td>-11</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 179 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    X1   X2   X3   X4   X5   X6   X7   X8   X9  X10 ...  X170  X171  X172  \\\n",
       "0  135  190  229  223  192  125   55   -9  -33  -38 ...   -17   -15   -31   \n",
       "1  386  382  356  331  320  315  307  272  244  232 ...   164   150   146   \n",
       "2  -32  -39  -47  -37  -32  -36  -57  -73  -85  -94 ...    57    64    48   \n",
       "3 -105 -101  -96  -92  -89  -95 -102 -100  -87  -79 ...   -82   -81   -80   \n",
       "4   -9  -65  -98 -102  -78  -48  -16    0  -21  -59 ...     4     2   -12   \n",
       "5   55   28   18   16   16   19   25   40   52   66 ...   -12   -31   -42   \n",
       "6  -55   -9   52  111  135  129  103   72   37    0 ...  -125   -99   -79   \n",
       "7    1   -2   -8  -11  -12  -17  -15  -16  -18  -17 ...   -79   -91   -97   \n",
       "8 -278 -246 -215 -191 -177 -167 -157 -139 -118  -92 ...  -400  -379  -336   \n",
       "9    8   15   13    3   -6   -8   -5    4   25   41 ...    49    31    11   \n",
       "\n",
       "   X173  X174  X175  X176  X177  X178  y  \n",
       "0   -77  -103  -127  -116   -83   -51  4  \n",
       "1   152   157   156   154   143   129  1  \n",
       "2    19   -12   -30   -35   -35   -36  5  \n",
       "3   -77   -85   -77   -72   -69   -65  5  \n",
       "4   -32   -41   -65   -83   -89   -73  5  \n",
       "5   -54   -60   -64   -60   -56   -55  5  \n",
       "6   -62   -41   -26    11    67   128  4  \n",
       "7   -88   -76   -72   -66   -57   -39  2  \n",
       "8  -281  -226  -174  -125   -79   -40  1  \n",
       "9    -5   -17   -19   -15   -15   -11  4  \n",
       "\n",
       "[10 rows x 179 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Reading the Epileptic Seizure Data Set\n",
    "dataset = pd.read_csv(\"seizure_edited.csv\", header = 0)\n",
    "print(dataset.shape)\n",
    "\n",
    "#split the dataset into features and labels\n",
    "labels = dataset['y']\n",
    "data = dataset.drop('y', axis = 1)\n",
    "\n",
    "dataset.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1: Data scaling & dimensionality reduction with PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both K-means and DBSCAN break down in high dimensions, as distance (and thus density) become less meaningful in high dimensions. So, before we run either of these clustering algorithms, we want to reduce the dimensionality of our dataset. Recall that it is critical to scale your data prior to running PCA. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q1. \n",
    "* Use `sklearn.preprocessing.StandardScaler` to standardize the dataset’s features (mean = 0 and variance = 1). Only standardize the the features, not the class labels! This will be required for running the principal component analysis (PCA) dimensionality reduction. Note that `StandardScaler` returns a numpy array.\n",
    "* Use `sklearn.decomposition.PCA` to perform PCA on the data.\n",
    "* Set `n_components` to 0.95 and `svd_solver` to 'full', so that the PCA will reduce the number of dimensions to however many are necessary to retail 95% of the variance. \n",
    "* Use `fit_transform` to perform the dimensionality reduction. \n",
    "* Note that `fit_transform` returns a numpy array. Put your transformed data back into a Pandas DataFrame by doing something like this (subsitute your variable names): `data = DataFrame(pca_data)`\n",
    "* Show the head of the resulting DataFrame (and make sure the dimenaionality has actually been reduced)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# your code goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q2. We have provided code here to plot the data. This code uses `sklearn.manifold.MDS` to project the data onto 2 dimensions. It first randomly samples 300 rows from the entire dataset, then runs MDS on this sample, and plots it on a scatter plot. The points will be marked with appropriate colors based on their labels. \n",
    "* Pass your dataset (that has been reduced in dimensions from Q1) and the labels into this scatter function. Verify that the plot looks like 2 clusters: an inner cluster (non-seizure patients, labels 2-5), and an outer cluster (seizure patients, label 1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# do not change this plotting function!\n",
    "def scatter(data, labels, numPoints = 300):\n",
    "    \n",
    "    numEntries = data.shape[0]\n",
    "    start = rand.randint(0, numEntries - numPoints)\n",
    "    end = start + numPoints\n",
    "    data = data.iloc[start:end, :]\n",
    "    labels = labels.iloc[start:end]\n",
    "    \n",
    "    mds = MDS(n_components=2)\n",
    "    mds_data = mds.fit_transform(data.iloc[:, :-1])\n",
    "    plt.scatter(mds_data[:, 0], mds_data[:, 1], c=labels, s=50)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# your code goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2: Clustering with DBSCAN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall that for DBSCAN, we need to select an Eps and a MinPts. If we choose MinPts to be 4, then Eps can be selected by plotting the distance of every point to its 4th nearest neighbor and finding the \"bend\" in the graph. \n",
    "\n",
    "Q3. Find the optimal Eps for this dataset by plotting the distances of every point to its 4th nearest neighbor.\n",
    "* Use `sklearn.neighbors.NearestNeighbors` with `n_neighbors=4`. Call `fit` on your data.\n",
    "* Use the `kneighbors` method to find the distance of each point to its 4th nearest neighbor. Note that `kneighbors` returns two values. The first return value is a matrix of distances from each point to its k nearest neighbors. For example: \n",
    "\n",
    "Sklearn considers the distance from the point to itself as the closest neighbor (a distance of 0), then if the point is a distance of 2.5 away from its second nearest neighbor, a distance of 3.7 away from its third nearest neighbor, and a distance of 4.2 away from its fourth nearest neighbor, the row in the distance matrix for this point would look like: `[0, 2.5, 3.7, 4.2]`, indicating that 4.2 is the distance from this point to it's 4th nearest neighbor. \n",
    "\n",
    "* Plot the distance of every point to its 4th nearest neighbor. Be sure to sort the distances before you plot them. You can use `plt.plot(distances)` then `plt.show()` to plot them. \n",
    "* Use the plot to determine the best Eps for this dataset (just eyeball it)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# your code goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q4. \n",
    "* Using MinPts=4 and Eps=the distance you determined from Q4, cluster the data using `sklearn.cluster.DBSCAN`. Call `fit_predict` on your data and store the return value (the predicted cluster label for each data point). Note that the returned type is a numpy array. \n",
    "* Use the provided `scatter` function (Q2) to plot the resulting clusters. You will need to cast the numpy array of predicted labels to a `Series` in order to pass it in to `scatter`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# your code goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 3: Clustering with K-means"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q5. \n",
    "* Use `sklearn.cluster.KMeans` to cluster this data using K means. Try values of k from 2 to 5. You will need to call `fit_predict` on your data and store the return value (the predicted cluster label for each data point). Note that the returned type is a numpy array. \n",
    "* Use `sklearn.metrics.silhouette_score` to evaluate which is the best number of clusters for this dataset. Note that you will need to cast the numpy array to a Series in order to pass it in to `silhouette_score`.\n",
    "* Use the provided `scatter` function (Q2) to plot the clustering that resulted in the best shilhouette score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# your code goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q6. What do you observe about the results of DBSCAN vs the results of K-means? Answer as a comment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Your answer goes here.\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 4: Hierarchical Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For hierarchical clustering (Q7 and Q8), you'll work with just a sample of 115 datapoints. The sampling code is provided here for you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(115, 40)\n"
     ]
    }
   ],
   "source": [
    "# Do not change the code in this cell\n",
    "# stratified sampling of the data\n",
    "\n",
    "def downsample_data(data):\n",
    "    downData = None\n",
    "    downLabels = None\n",
    "\n",
    "    kfold = StratifiedKFold(n_splits = 100, shuffle = True)\n",
    "    for throwAway_index, subsample_index in kfold.split(data, labels):\n",
    "        downData = data.iloc[subsample_index]\n",
    "        downLabels = labels.iloc[subsample_index]\n",
    "        break\n",
    "\n",
    "    print(downData.shape)\n",
    "    return downData"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q7. Use Scipy's hierarchical clustering (`scipy.cluster.hierarchy`) to cluster the data using different linkage methods. **Make sure you are using a down-sampled dataset by passing your dataset into the provided downsample_data() function.**\n",
    "* Try all of these linkage methods: `single`, `complete`, `average`, `centroid`, `ward`\n",
    "* Note that the return value of the clustering is a linkage matrix (a cophenetic distance matrix)\n",
    "\n",
    "* Calculate the CPCC of each linkage method to determine which method worked best. Use `scipy.cluster.hierarchy.cophenet` for this. You'll pass in the linkage matrix and a proximity matrix that contains the actual distances between each point. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Make sure to pass your dataset into downsample_data() \n",
    "# and use the resulting smaller dataset for hierachical clustering\n",
    "\n",
    "# Pass the down-sampled dataset into pdist to get your proximity matrix for calculating CPCC\n",
    "proximity_matrix = pdist(sampled_dataset_goes_here)\n",
    "\n",
    "# the rest of your code goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q8. Using the best linkage method that you found in Q7, plot the dendrogram with `scipy.cluster.hierarchy.dendrogram`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# your code goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 5: Anomaly Detection\n",
    "\n",
    "In most real life applications the data observations are subject to noise present either in the environment or the sensor apparatus. Anomaly detection (AD) plays a vital role in the preprocessing stage to identify these outliers and optionally remove them from the data set. Anomaly detection algorithms can be broadly classified into 3 classes (see Nicolas Goix's [slides](https://ngoix.github.io/nicolas_goix_osi_presentation.pdf) on anomaly detection):\n",
    "- Supervised AD (labels available for both normal data and anomalies)\n",
    "- Semi-supervised AD, also called Novelty Detection (only normal data available to train)\n",
    "- Unsupervised AD, also called Outlier Detection (no labels are provided)\n",
    "\n",
    "In this part you'll experiment with 4 datasets and compare 4 different outlier detection (Unsupervised AD) methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data1.csv (445, 2)\n",
      "data2.csv (314, 2)\n",
      "data3.csv (419, 2)\n",
      "data4.csv (441, 2)\n"
     ]
    }
   ],
   "source": [
    "# loading the datasets\n",
    "filenames = ['data1.csv', 'data2.csv', 'data3.csv', 'data4.csv']\n",
    "datasets = [np.loadtxt(fname, delimiter=', ') for fname in filenames]\n",
    "for fname, data in zip(filenames, datasets):\n",
    "    print (fname, data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q10. Detect the outliers in all 4 datasets using the following algorithms:\n",
    "- `sklearn.covariance.EllipticEnvelope` - a statistical-based AD method\n",
    "- `sklearn.ensemble.IsolationForest` - an isolation-based AD method\n",
    "- `sklearn.neighbors.LocalOutlierFactor` - a density-based AD method\n",
    "- `sklearn.svm.OneClassSVM` - a model-based AD method\n",
    "\n",
    "Create scatterplots that show the outliers detected in each dataset with each method. (16 plots in total.)\n",
    "For example:\n",
    "![title](scatters.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following plotting function is provided for you. You should pass in a list of `y_pred` values corresponding to all 16 of your plots. In order for this plotting function to work correctly, the order of your list should be: ElipticEnvelope on dataset1, IsolationForest on dataset1, LocalOutlierFactor on dataset1, One-Class SVM on dataset1, ElipticEnvelope on dataset 2, IsolationForest on ds2, LOF on ds2, SVM on ds2, ElipticEnvelope on dataset3, IsolationForest on ds3, LOF on ds3, SVM on ds3, EE on dataset4, IF on ds4, LOF on ds4, SVM on ds4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Do not change code in this cell!\n",
    "\n",
    "def plot_all(y_preds):\n",
    "    \n",
    "    print(\"len(y_preds):\", len(y_preds))\n",
    "    \n",
    "    names = ['Robust Covariance', 'Isolation Forest', 'Local Outlier Factor', 'One-Class SVM']\n",
    "    plt.figure(figsize=(len(names) * 2 + 3, 12.5))\n",
    "    plt.subplots_adjust(left=.02, right=.98, bottom=.001, top=.96, wspace=.05, hspace=.01)\n",
    "    plot_num = 1\n",
    "    \n",
    "    for i_dataset in range(len(datasets)):\n",
    "        for j_algorithm in range(len(names)):\n",
    "            \n",
    "            plt.subplot(len(datasets), len(names), plot_num)\n",
    "            if i_dataset == 0:\n",
    "                plt.title(names[j_algorithm], size=18)\n",
    "\n",
    "            colors = np.array(['#377eb8', '#ff7f00'])\n",
    "            plt.scatter(datasets[i_dataset][:, 0], datasets[i_dataset][:, 1], \n",
    "                        s=10, color=colors[(y_preds[plot_num-1] + 1) // 2])\n",
    "\n",
    "            plt.xlim(-7, 7)\n",
    "            plt.ylim(-7, 7)\n",
    "            plt.xticks(())\n",
    "            plt.yticks(())\n",
    "            \n",
    "            plot_num += 1\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Directions:\n",
    "* Use a `contamination` (or `nu` for `OneClassSVM`) of 0.25.\n",
    "* For the `OneClassSVM` use `rbf` kernel.\n",
    "* NOTE: To preform anomaly detection in an unsupervised way, you will pass the same dataset into `fit` and `predict.`\n",
    "\n",
    "Some code is provided here to get you started."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.covariance import EllipticEnvelope\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "from sklearn.svm import OneClassSVM\n",
    "\n",
    "\n",
    "outliers_fraction = 0.25\n",
    "\n",
    "#this will be your list of 16 plots\n",
    "y_pred_list = list()\n",
    "\n",
    "\n",
    "for X in datasets:\n",
    "    #apply EllipticEnvelope AD\n",
    "    #append resulting y_pred to y_pred_list\n",
    "    \n",
    "    #apply IsolationForest AD\n",
    "    #append resulting y_pred to y_pred_list\n",
    "    \n",
    "    #apply LocalOutlierFactor AD\n",
    "    #append resulting y_pred to y_pred_list\n",
    "\n",
    "    #apply OneClassSVM AD\n",
    "    #append resulting y_pred to y_pred_list\n",
    "\n",
    " \n",
    "\n",
    "plot_all(y_pred_list)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
