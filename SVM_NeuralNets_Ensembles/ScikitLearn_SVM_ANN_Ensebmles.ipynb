{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial 4 : SVMs, Neural Nets, Ensembles\n",
    "\n",
    "In this tutorial you'll implement SVMs, Neural Nets, and Ensembling methods to classify patients as either having or not having diabetic retinopathy. For this task we'll be using the same Diabetic Retinopathy data set which was used in the previous assignments. You can find additional details about the dataset [here](http://archive.ics.uci.edu/ml/datasets/Diabetic+Retinopathy+Debrecen+Data+Set). You'll explore how to train SVMs, NNs, and Ensembles using the `scikit-learn` library. The scikit-learn documentation can be found [here](http://scikit-learn.org/stable/documentation.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ykass\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\weight_boosting.py:29: DeprecationWarning: numpy.core.umath_tests is an internal NumPy module and should not be imported. It will be removed in a future NumPy release.\n",
      "  from numpy.core.umath_tests import inner1d\n"
     ]
    }
   ],
   "source": [
    "#You may add additional imports\n",
    "import warnings\n",
    "warnings.simplefilter(\"ignore\")\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn as sk\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.ensemble import AdaBoostClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1151, 20)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>quality</th>\n",
       "      <th>prescreen</th>\n",
       "      <th>ma2</th>\n",
       "      <th>ma3</th>\n",
       "      <th>ma4</th>\n",
       "      <th>ma5</th>\n",
       "      <th>ma6</th>\n",
       "      <th>ma7</th>\n",
       "      <th>exudate8</th>\n",
       "      <th>exudate9</th>\n",
       "      <th>exudate10</th>\n",
       "      <th>exudate11</th>\n",
       "      <th>exudate12</th>\n",
       "      <th>exudate13</th>\n",
       "      <th>exudate14</th>\n",
       "      <th>exudate15</th>\n",
       "      <th>euDist</th>\n",
       "      <th>diameter</th>\n",
       "      <th>amfm_class</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>22</td>\n",
       "      <td>22</td>\n",
       "      <td>22</td>\n",
       "      <td>19</td>\n",
       "      <td>18</td>\n",
       "      <td>14</td>\n",
       "      <td>49.895756</td>\n",
       "      <td>17.775994</td>\n",
       "      <td>5.270920</td>\n",
       "      <td>0.771761</td>\n",
       "      <td>0.018632</td>\n",
       "      <td>0.006864</td>\n",
       "      <td>0.003923</td>\n",
       "      <td>0.003923</td>\n",
       "      <td>0.486903</td>\n",
       "      <td>0.100025</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>24</td>\n",
       "      <td>24</td>\n",
       "      <td>22</td>\n",
       "      <td>18</td>\n",
       "      <td>16</td>\n",
       "      <td>13</td>\n",
       "      <td>57.709936</td>\n",
       "      <td>23.799994</td>\n",
       "      <td>3.325423</td>\n",
       "      <td>0.234185</td>\n",
       "      <td>0.003903</td>\n",
       "      <td>0.003903</td>\n",
       "      <td>0.003903</td>\n",
       "      <td>0.003903</td>\n",
       "      <td>0.520908</td>\n",
       "      <td>0.144414</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>62</td>\n",
       "      <td>60</td>\n",
       "      <td>59</td>\n",
       "      <td>54</td>\n",
       "      <td>47</td>\n",
       "      <td>33</td>\n",
       "      <td>55.831441</td>\n",
       "      <td>27.993933</td>\n",
       "      <td>12.687485</td>\n",
       "      <td>4.852282</td>\n",
       "      <td>1.393889</td>\n",
       "      <td>0.373252</td>\n",
       "      <td>0.041817</td>\n",
       "      <td>0.007744</td>\n",
       "      <td>0.530904</td>\n",
       "      <td>0.128548</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>55</td>\n",
       "      <td>53</td>\n",
       "      <td>53</td>\n",
       "      <td>50</td>\n",
       "      <td>43</td>\n",
       "      <td>31</td>\n",
       "      <td>40.467228</td>\n",
       "      <td>18.445954</td>\n",
       "      <td>9.118901</td>\n",
       "      <td>3.079428</td>\n",
       "      <td>0.840261</td>\n",
       "      <td>0.272434</td>\n",
       "      <td>0.007653</td>\n",
       "      <td>0.001531</td>\n",
       "      <td>0.483284</td>\n",
       "      <td>0.114790</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>44</td>\n",
       "      <td>44</td>\n",
       "      <td>44</td>\n",
       "      <td>41</td>\n",
       "      <td>39</td>\n",
       "      <td>27</td>\n",
       "      <td>18.026254</td>\n",
       "      <td>8.570709</td>\n",
       "      <td>0.410381</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.475935</td>\n",
       "      <td>0.123572</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>44</td>\n",
       "      <td>43</td>\n",
       "      <td>41</td>\n",
       "      <td>41</td>\n",
       "      <td>37</td>\n",
       "      <td>29</td>\n",
       "      <td>28.356400</td>\n",
       "      <td>6.935636</td>\n",
       "      <td>2.305771</td>\n",
       "      <td>0.323724</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.502831</td>\n",
       "      <td>0.126741</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>29</td>\n",
       "      <td>29</td>\n",
       "      <td>29</td>\n",
       "      <td>27</td>\n",
       "      <td>25</td>\n",
       "      <td>16</td>\n",
       "      <td>15.448398</td>\n",
       "      <td>9.113819</td>\n",
       "      <td>1.633493</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.541743</td>\n",
       "      <td>0.139575</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>20.679649</td>\n",
       "      <td>9.497786</td>\n",
       "      <td>1.223660</td>\n",
       "      <td>0.150382</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.576318</td>\n",
       "      <td>0.071071</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>22</td>\n",
       "      <td>21</td>\n",
       "      <td>18</td>\n",
       "      <td>15</td>\n",
       "      <td>13</td>\n",
       "      <td>10</td>\n",
       "      <td>66.691933</td>\n",
       "      <td>23.545543</td>\n",
       "      <td>6.151117</td>\n",
       "      <td>0.496372</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.500073</td>\n",
       "      <td>0.116793</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>79</td>\n",
       "      <td>75</td>\n",
       "      <td>73</td>\n",
       "      <td>71</td>\n",
       "      <td>64</td>\n",
       "      <td>47</td>\n",
       "      <td>22.141784</td>\n",
       "      <td>10.054384</td>\n",
       "      <td>0.874633</td>\n",
       "      <td>0.099780</td>\n",
       "      <td>0.023386</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.560959</td>\n",
       "      <td>0.109134</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   quality  prescreen  ma2  ma3  ma4  ma5  ma6  ma7   exudate8   exudate9  \\\n",
       "0        1          1   22   22   22   19   18   14  49.895756  17.775994   \n",
       "1        1          1   24   24   22   18   16   13  57.709936  23.799994   \n",
       "2        1          1   62   60   59   54   47   33  55.831441  27.993933   \n",
       "3        1          1   55   53   53   50   43   31  40.467228  18.445954   \n",
       "4        1          1   44   44   44   41   39   27  18.026254   8.570709   \n",
       "5        1          1   44   43   41   41   37   29  28.356400   6.935636   \n",
       "6        1          0   29   29   29   27   25   16  15.448398   9.113819   \n",
       "7        1          1    6    6    6    6    2    1  20.679649   9.497786   \n",
       "8        1          1   22   21   18   15   13   10  66.691933  23.545543   \n",
       "9        1          1   79   75   73   71   64   47  22.141784  10.054384   \n",
       "\n",
       "   exudate10  exudate11  exudate12  exudate13  exudate14  exudate15    euDist  \\\n",
       "0   5.270920   0.771761   0.018632   0.006864   0.003923   0.003923  0.486903   \n",
       "1   3.325423   0.234185   0.003903   0.003903   0.003903   0.003903  0.520908   \n",
       "2  12.687485   4.852282   1.393889   0.373252   0.041817   0.007744  0.530904   \n",
       "3   9.118901   3.079428   0.840261   0.272434   0.007653   0.001531  0.483284   \n",
       "4   0.410381   0.000000   0.000000   0.000000   0.000000   0.000000  0.475935   \n",
       "5   2.305771   0.323724   0.000000   0.000000   0.000000   0.000000  0.502831   \n",
       "6   1.633493   0.000000   0.000000   0.000000   0.000000   0.000000  0.541743   \n",
       "7   1.223660   0.150382   0.000000   0.000000   0.000000   0.000000  0.576318   \n",
       "8   6.151117   0.496372   0.000000   0.000000   0.000000   0.000000  0.500073   \n",
       "9   0.874633   0.099780   0.023386   0.000000   0.000000   0.000000  0.560959   \n",
       "\n",
       "   diameter  amfm_class  label  \n",
       "0  0.100025           1      0  \n",
       "1  0.144414           0      0  \n",
       "2  0.128548           0      1  \n",
       "3  0.114790           0      0  \n",
       "4  0.123572           0      1  \n",
       "5  0.126741           0      1  \n",
       "6  0.139575           0      1  \n",
       "7  0.071071           1      0  \n",
       "8  0.116793           0      1  \n",
       "9  0.109134           0      1  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read the data from csv file\n",
    "col_names = []\n",
    "for i in range(20):\n",
    "    if i == 0:\n",
    "        col_names.append('quality')\n",
    "    if i == 1:\n",
    "        col_names.append('prescreen')\n",
    "    if i >= 2 and i <= 7:\n",
    "        col_names.append('ma' + str(i))\n",
    "    if i >= 8 and i <= 15:\n",
    "        col_names.append('exudate' + str(i))\n",
    "    if i == 16:\n",
    "        col_names.append('euDist')\n",
    "    if i == 17:\n",
    "        col_names.append('diameter')\n",
    "    if i == 18:\n",
    "        col_names.append('amfm_class')\n",
    "    if i == 19:\n",
    "        col_names.append('label')\n",
    "\n",
    "data = pd.read_csv(\"messidor_features.txt\", names = col_names)\n",
    "print(data.shape)\n",
    "data.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Data prep"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q1. Separate the feature columns from the class label column. You should end up with two separate data frames - one that contains all of the feature values and one that contains the class labels. Print the shape of the features DataFrame, the shape of the labels DataFrame, and the head of the features DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1151, 19)\n",
      "(1151, 1)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>quality</th>\n",
       "      <th>prescreen</th>\n",
       "      <th>ma2</th>\n",
       "      <th>ma3</th>\n",
       "      <th>ma4</th>\n",
       "      <th>ma5</th>\n",
       "      <th>ma6</th>\n",
       "      <th>ma7</th>\n",
       "      <th>exudate8</th>\n",
       "      <th>exudate9</th>\n",
       "      <th>exudate10</th>\n",
       "      <th>exudate11</th>\n",
       "      <th>exudate12</th>\n",
       "      <th>exudate13</th>\n",
       "      <th>exudate14</th>\n",
       "      <th>exudate15</th>\n",
       "      <th>euDist</th>\n",
       "      <th>diameter</th>\n",
       "      <th>amfm_class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>22</td>\n",
       "      <td>22</td>\n",
       "      <td>22</td>\n",
       "      <td>19</td>\n",
       "      <td>18</td>\n",
       "      <td>14</td>\n",
       "      <td>49.895756</td>\n",
       "      <td>17.775994</td>\n",
       "      <td>5.270920</td>\n",
       "      <td>0.771761</td>\n",
       "      <td>0.018632</td>\n",
       "      <td>0.006864</td>\n",
       "      <td>0.003923</td>\n",
       "      <td>0.003923</td>\n",
       "      <td>0.486903</td>\n",
       "      <td>0.100025</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>24</td>\n",
       "      <td>24</td>\n",
       "      <td>22</td>\n",
       "      <td>18</td>\n",
       "      <td>16</td>\n",
       "      <td>13</td>\n",
       "      <td>57.709936</td>\n",
       "      <td>23.799994</td>\n",
       "      <td>3.325423</td>\n",
       "      <td>0.234185</td>\n",
       "      <td>0.003903</td>\n",
       "      <td>0.003903</td>\n",
       "      <td>0.003903</td>\n",
       "      <td>0.003903</td>\n",
       "      <td>0.520908</td>\n",
       "      <td>0.144414</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>62</td>\n",
       "      <td>60</td>\n",
       "      <td>59</td>\n",
       "      <td>54</td>\n",
       "      <td>47</td>\n",
       "      <td>33</td>\n",
       "      <td>55.831441</td>\n",
       "      <td>27.993933</td>\n",
       "      <td>12.687485</td>\n",
       "      <td>4.852282</td>\n",
       "      <td>1.393889</td>\n",
       "      <td>0.373252</td>\n",
       "      <td>0.041817</td>\n",
       "      <td>0.007744</td>\n",
       "      <td>0.530904</td>\n",
       "      <td>0.128548</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>55</td>\n",
       "      <td>53</td>\n",
       "      <td>53</td>\n",
       "      <td>50</td>\n",
       "      <td>43</td>\n",
       "      <td>31</td>\n",
       "      <td>40.467228</td>\n",
       "      <td>18.445954</td>\n",
       "      <td>9.118901</td>\n",
       "      <td>3.079428</td>\n",
       "      <td>0.840261</td>\n",
       "      <td>0.272434</td>\n",
       "      <td>0.007653</td>\n",
       "      <td>0.001531</td>\n",
       "      <td>0.483284</td>\n",
       "      <td>0.114790</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>44</td>\n",
       "      <td>44</td>\n",
       "      <td>44</td>\n",
       "      <td>41</td>\n",
       "      <td>39</td>\n",
       "      <td>27</td>\n",
       "      <td>18.026254</td>\n",
       "      <td>8.570709</td>\n",
       "      <td>0.410381</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.475935</td>\n",
       "      <td>0.123572</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>44</td>\n",
       "      <td>43</td>\n",
       "      <td>41</td>\n",
       "      <td>41</td>\n",
       "      <td>37</td>\n",
       "      <td>29</td>\n",
       "      <td>28.356400</td>\n",
       "      <td>6.935636</td>\n",
       "      <td>2.305771</td>\n",
       "      <td>0.323724</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.502831</td>\n",
       "      <td>0.126741</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>29</td>\n",
       "      <td>29</td>\n",
       "      <td>29</td>\n",
       "      <td>27</td>\n",
       "      <td>25</td>\n",
       "      <td>16</td>\n",
       "      <td>15.448398</td>\n",
       "      <td>9.113819</td>\n",
       "      <td>1.633493</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.541743</td>\n",
       "      <td>0.139575</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>20.679649</td>\n",
       "      <td>9.497786</td>\n",
       "      <td>1.223660</td>\n",
       "      <td>0.150382</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.576318</td>\n",
       "      <td>0.071071</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>22</td>\n",
       "      <td>21</td>\n",
       "      <td>18</td>\n",
       "      <td>15</td>\n",
       "      <td>13</td>\n",
       "      <td>10</td>\n",
       "      <td>66.691933</td>\n",
       "      <td>23.545543</td>\n",
       "      <td>6.151117</td>\n",
       "      <td>0.496372</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.500073</td>\n",
       "      <td>0.116793</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>79</td>\n",
       "      <td>75</td>\n",
       "      <td>73</td>\n",
       "      <td>71</td>\n",
       "      <td>64</td>\n",
       "      <td>47</td>\n",
       "      <td>22.141784</td>\n",
       "      <td>10.054384</td>\n",
       "      <td>0.874633</td>\n",
       "      <td>0.099780</td>\n",
       "      <td>0.023386</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.560959</td>\n",
       "      <td>0.109134</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   quality  prescreen  ma2  ma3  ma4  ma5  ma6  ma7   exudate8   exudate9  \\\n",
       "0        1          1   22   22   22   19   18   14  49.895756  17.775994   \n",
       "1        1          1   24   24   22   18   16   13  57.709936  23.799994   \n",
       "2        1          1   62   60   59   54   47   33  55.831441  27.993933   \n",
       "3        1          1   55   53   53   50   43   31  40.467228  18.445954   \n",
       "4        1          1   44   44   44   41   39   27  18.026254   8.570709   \n",
       "5        1          1   44   43   41   41   37   29  28.356400   6.935636   \n",
       "6        1          0   29   29   29   27   25   16  15.448398   9.113819   \n",
       "7        1          1    6    6    6    6    2    1  20.679649   9.497786   \n",
       "8        1          1   22   21   18   15   13   10  66.691933  23.545543   \n",
       "9        1          1   79   75   73   71   64   47  22.141784  10.054384   \n",
       "\n",
       "   exudate10  exudate11  exudate12  exudate13  exudate14  exudate15    euDist  \\\n",
       "0   5.270920   0.771761   0.018632   0.006864   0.003923   0.003923  0.486903   \n",
       "1   3.325423   0.234185   0.003903   0.003903   0.003903   0.003903  0.520908   \n",
       "2  12.687485   4.852282   1.393889   0.373252   0.041817   0.007744  0.530904   \n",
       "3   9.118901   3.079428   0.840261   0.272434   0.007653   0.001531  0.483284   \n",
       "4   0.410381   0.000000   0.000000   0.000000   0.000000   0.000000  0.475935   \n",
       "5   2.305771   0.323724   0.000000   0.000000   0.000000   0.000000  0.502831   \n",
       "6   1.633493   0.000000   0.000000   0.000000   0.000000   0.000000  0.541743   \n",
       "7   1.223660   0.150382   0.000000   0.000000   0.000000   0.000000  0.576318   \n",
       "8   6.151117   0.496372   0.000000   0.000000   0.000000   0.000000  0.500073   \n",
       "9   0.874633   0.099780   0.023386   0.000000   0.000000   0.000000  0.560959   \n",
       "\n",
       "   diameter  amfm_class  \n",
       "0  0.100025           1  \n",
       "1  0.144414           0  \n",
       "2  0.128548           0  \n",
       "3  0.114790           0  \n",
       "4  0.123572           0  \n",
       "5  0.126741           0  \n",
       "6  0.139575           0  \n",
       "7  0.071071           1  \n",
       "8  0.116793           0  \n",
       "9  0.109134           0  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create new dataframe with the feature values columns\n",
    "feature_data = data.drop(['label'],axis=1)\n",
    "# create new dataframe with the class label values columns\n",
    "label_data = data[['label']].copy()\n",
    "# print the shapes of the new dataframes\n",
    "print(feature_data.shape)\n",
    "print(label_data.shape)\n",
    "# print head of the features DataFrame\n",
    "feature_data.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Support Vector Machines (SVM) and Pipelines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q2. For some classification algorithms, like KNN, SVMs, and Neural Nets, scaling of the data is critical for the algorithm to operate correctly. For other classification algorithms, like Naive Bayes, and Decision Trees, data scaling is not necessary (take a minute to think about why that is the case). \n",
    "\n",
    "We discussed in class how the data scaling should happen on the _training set only_, which means that it should happen _inside_ of the cross validation loop. In other words, in each fold of the cross validation, the data will be separated in to training and test sets. The scaling (calculating mean and std, for instance) should happen based on the values in the _traning set only_. Then the test set can be scaled using the values found on the training set. (Refer to the concept of [data leakage](https://machinelearningmastery.com/data-leakage-machine-learning/).)\n",
    "\n",
    "In order to do this with scikit-learn, you must create what's called a `Pipeline` and pass that in to the cross validation. This is a very important concept for Data Mining and Machine Learning, so let's practice it here.\n",
    "\n",
    "Do the following:\n",
    "* Create a `sklearn.preprocessing.StandardScaler` object to standardize the datasetâ€™s features (mean = 0 and variance = 1). Do not call `fit` on it yet. Just create the `StandardScaler` object.\n",
    "* Create a `sklearn.svm.SVC` classifier with a `linear` kernel. Do not call `fit` on it yet. Just create the `SVC` object.\n",
    "* Create a `sklearn.pipeline.Pipeline` and set the `steps` to the scaler and the SVC objects that you just created. \n",
    "* Pass the `pipeline` in to a `cross_val_score` as the estimator, along with the features and the labels, and use a 5-fold-CV. \n",
    "\n",
    "In each fold of the cross validation, the training phase will use _only_ the training data for scaling and training the model. Then the testing phase will scale the test data into the scaled space (found on the training data) and run the test data through the trained classifier, to return an accuracy measurement for each fold. Print the average accuracy across all 5 folds. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 72.28646715603239\n"
     ]
    }
   ],
   "source": [
    "# create standard scaler object\n",
    "scaler = StandardScaler()\n",
    "# create svc object\n",
    "svc = SVC(kernel='linear')\n",
    "# setup pipeline\n",
    "pipeline = Pipeline(steps=[('scaler', scaler), ('svc', svc)])\n",
    "# run pipeline with cvs and get accuracy\n",
    "piped_cvs = cross_val_score(pipeline, feature_data, label_data, cv=5)\n",
    "print(\"Accuracy:\", piped_cvs.mean()*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q3. The `svm.SVC` defaults to using an rbf (radial basis function) kernel. This kernel may or may not be the best choice for our dataset. We can use nested cross validation to find the best kernel for this dataset.\n",
    "\n",
    "Set up the inner CV loop:\n",
    "* Starter code is provided to create the \"parameter grid\" to search. You will need to change this code! Where I have \"svm__kernel\", this indicates that I want to tune the \"kernel\" parameter in the \"svm\" part of the pipeline. When you created your pipeline above, you named the SVM part of the pipeline with a string. You should replace \"svm\" in the param_grid below with whatever you named your SVM part of the pipeline: **<replace_this>__kernel.** \n",
    "* Create a `sklearn.model_selection.GridSearchCV` that takes in the pipeline you created above (as the estimator), the parameter grid, and uses a 5-fold-CV. Call `fit` on the `GridSearchCV` to find the best kernel. \n",
    "* Print out the best kernel (`best_params_`) for this dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'svc__kernel': 'linear'}\n"
     ]
    }
   ],
   "source": [
    "# for the 'svm' part of the pipeline, tune the 'kernel' hyperparameter\n",
    "param_grid = {'svc__kernel': ['linear', 'rbf', 'poly', 'sigmoid']}\n",
    "\n",
    "# use GridSearchCV on the pipeline\n",
    "piped_grid = GridSearchCV(pipeline, param_grid, cv=5, scoring='accuracy')\n",
    "# call fit() on the GridSearchCV and pass in the data\n",
    "piped_grid.fit(feature_data, label_data)\n",
    "# print out best_params_ from the GridSearchCV\n",
    "print(piped_grid.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q4. Now put what you did in Q3 in to an outer CV loop to evaluate the accuracy of using that best-found kernel on unseen test data. \n",
    "* Pass the `GridSearchCV` in to a `cross_val_score` with 5-fold-CV. Print out the accuracy.\n",
    "\n",
    "Note that the accuracy increases with a better choice of kernel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 72.28646715603239\n"
     ]
    }
   ],
   "source": [
    "# pass GrisSearchCV from q3 in to a cross val\n",
    "outer_cv_grid = cross_val_score(piped_grid, feature_data, label_data, cv=5)\n",
    "print(\"Accuracy:\", outer_cv_grid.mean()*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q5. Let's see if we can get the accuracy even higher by tuning additional hyperparameters. SVMs have a parameter called 'C' that is the cost for a misclassification. (More info [here](https://medium.com/@pushkarmandot/what-is-the-significance-of-c-value-in-support-vector-machine-28224e852c5a)).\n",
    "* Create a parameter grid that includes the kernel (as you have above) and the C value as well. Try values of C from 50 to 100 by increments of 10. (You can use the range function to help you with this.)\n",
    "* Create a `GridSearchCV` with the pipeline from above, this new parameter grid, and a 5-fold-CV.\n",
    "* Pass the `GridSearchCV` into a `cross_val_score` with a 5-fold-CV and print out the accuracy.\n",
    "\n",
    "Be patient as this can take some time to run. Note that the accurcay has increased even further because the best value of C was found and used on the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 74.54357236965933\n"
     ]
    }
   ],
   "source": [
    "# create paramter grid with kernel and C vals\n",
    "param_grid2 = {'svc__kernel': ['linear'],'svc__C': [50.0, 60.0, 70.0, 80.0, 90.0, 100.0]}\n",
    "# create gridsearch with pipeline and params\n",
    "piped_grid2 = GridSearchCV(pipeline, param_grid2, cv=5, scoring='accuracy')\n",
    "piped_grid2.fit(feature_data, label_data)\n",
    "# pass the gridsearch into a cvs and print accuracy\n",
    "outer_cv_grid2 = cross_val_score(piped_grid2, feature_data, label_data, cv=5)\n",
    "print(\"Accuracy:\", outer_cv_grid2.mean()*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Neural Networks (NN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q6. Train a multi-layer perceptron with a single hidden layer using `sklearn.neural_network.MLPClassifier`. \n",
    "* Scaling is critical to neural networks. Create a pipeline that includes scaling and an `MLPClassifier`.\n",
    "* Use `GridSearchCV` with 5 folds to find the best hidden layer size and the best activation function. \n",
    "* Try values of `hidden_layer_sizes` ranging from `(10,)` to `(60,)` with gaps of 10.\n",
    "* Try activation functions `logistic`, `tanh`, `relu`.\n",
    "\n",
    "Wrap your `GridSearchCV` in a 5-fold `cross_val_score` and report the accuracy of your neural net.\n",
    "\n",
    "Be patient, as this can take a few minutes to run. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 73.32881611142481\n"
     ]
    }
   ],
   "source": [
    "# make a pipeline with scaling and MLPClassifier\n",
    "mlp = MLPClassifier()\n",
    "nn_pipeline = Pipeline(steps=[('scaler', scaler), ('mlpclassifier', mlp)])\n",
    "# use gridsearch to find best Hidden Layer Size and Activation Function\n",
    "nn_grid = {'mlpclassifier__hidden_layer_sizes': [(10,), (20,), (30,), (40,),(50,),(60,)], 'mlpclassifier__activation': [\"logistic\", \"relu\", \"tanh\"]}\n",
    "nn_piped_grid = GridSearchCV(nn_pipeline, nn_grid, cv=5, scoring='accuracy')\n",
    "nn_piped_grid.fit(feature_data, label_data)\n",
    "# wrap gridsearch into a 5-fold cvs\n",
    "nn_cvs = cross_val_score(nn_piped_grid, feature_data, label_data, cv=5)\n",
    "print(\"Accuracy:\", nn_cvs.mean()*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Ensemble Classifiers\n",
    "\n",
    "Ensemble classifiers combine the predictions of multiple base estimators to improve the accuracy of the predictions. One of the key assumptions that ensemble classifiers make is that the base estimators are built independently (so they are diverse)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**A. Random Forests**\n",
    "\n",
    "Q7. Use `sklearn.ensemble.RandomForestClassifier` to classify the data. Use a `GridSearchCV` to tune the hyperparameters to get the best results. \n",
    "* Try `max_depth` ranging from 35-55 (you can use the range function to help you with this)\n",
    "* Try `min_samples_leaf` of 8, 10, 12\n",
    "* Try `max_features` of `\"sqrt\"` and `\"log2\"`\n",
    "\n",
    "Wrap your GridSearchCV in a cross_val_score with 5-fold CV to report the accuracy of the model.\n",
    "\n",
    "Be patient, this can take a few minutes to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 66.72576698663654\n"
     ]
    }
   ],
   "source": [
    "# startup random forest\n",
    "forest = RandomForestClassifier()\n",
    "# setup gridsearchcv\n",
    "forest_params = {'max_depth': [*range(1,11)], 'min_samples_leaf': [8,10,12], 'max_features':['sqrt','log2']}\n",
    "forest_grid = GridSearchCV(forest, forest_params)\n",
    "forest_grid.fit(feature_data, label_data)\n",
    "# wrap gridsearch into a 5-flod cvs\n",
    "forest_cvs = cross_val_score(forest_grid, feature_data, label_data, cv=5)\n",
    "print(\"Accuracy:\", forest_cvs.mean()*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**B. AdaBoost**\n",
    "\n",
    "Random Forests are a kind of ensemble classifier where many estimators are built independently in parallel. In contrast, there is another method of creating an ensemble classifier called *boosting*. Here the classifiers are trained one-by-one in sequence and each time the sampling of the training set depends on the performance of previously generated models.\n",
    "\n",
    "Q8. Evaluate a `sklearn.ensemble.AdaBoostClassifier` classifier on the data. By default, `AdaBoostClassifier` uses decision trees as the base classifiers (but this can be changed). \n",
    "* Use a GridSearchCV to find the best number of trees in the ensemble (`n_estimators`). Try values from 50-250 with increments of 25. (you can use the range function to help you with this.)\n",
    "* Wrap your GridSearchCV in a cross_val_score with 5-fold CV to report the accuracy of the model.\n",
    "\n",
    "Be patient, this can take a few minutes to run.\n",
    "\n",
    "Use 150 base decision tree classifiers to make an `AdaBoostClassifier` and evaluate it's accuracy with a 5-fold-CV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 70.9828722002635\n"
     ]
    }
   ],
   "source": [
    "# setup AdaBoost \n",
    "boost = AdaBoostClassifier()\n",
    "# do gridsearch\n",
    "boost_params = {'n_estimators': [*range(50,251,25)]}\n",
    "boost_grid = GridSearchCV(boost, boost_params)\n",
    "boost_grid.fit(feature_data, label_data)\n",
    "# do cvs with gridsearch and print accuracy\n",
    "boost_cvs = cross_val_score(boost_grid, feature_data, label_data, cv=5)\n",
    "print(\"Accuracy:\", boost_cvs.mean()*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Deploying a final model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Over the course of three programming assignments, you have tested all kinds of classifiers on this data. Some have performed better than others. \n",
    "\n",
    "We could continue trying to improve the accuracy of our models by tweaking their parameters more and/or we could do some feature engineering on our dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q9. Let's say we got to the point where we had a model with high accuracy and we want to deploy that model and use it for real-world predictions.\n",
    "\n",
    "* Let's say we're going to deploy our neural net classifier.\n",
    "* We need to make one final version of this model, where we use ALL of our available data for training (we do not hold out a test set this time, so no outer cross-validation loop). \n",
    "* We need to tune the parameters of the model on the FULL dataset, so copy the code you entered for Q6, but remove the outer cross validation loop (remove `cross_val_score`). Just run the `GridSearchCV` by calling `fit` on it and passing in the full dataset. This results in the final trained model with the best parameters for the full dataset. You can print out `best_params_` to see what they are.\n",
    "* The accuracy of this model is what you assessed and reported in Q6.\n",
    "\n",
    "\n",
    "* Use the `pickle` package to save your model. We have provided the lines of code for you, just make sure your final model gets passed in to `pickle.dump()`. This will save your model to a file called finalized_model.sav in your current working directory. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'mlpclassifier__activation': 'relu', 'mlpclassifier__hidden_layer_sizes': (50,)}\n",
      "0.735881841876629\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "#replace this final_model with your final model\n",
    "mlp = MLPClassifier()\n",
    "nn_pipeline = Pipeline(steps=[('scaler', scaler), ('mlpclassifier', mlp)])\n",
    "# use gridsearch to find best Hidden Layer Size and Activation Function\n",
    "nn_grid = {'mlpclassifier__hidden_layer_sizes': [(10,), (20,), (30,), (40,),(50,),(60,)], 'mlpclassifier__activation': [\"logistic\", \"relu\", \"tanh\"]}\n",
    "nn_piped_grid = GridSearchCV(nn_pipeline, nn_grid, cv=5, scoring='accuracy')\n",
    "final_model = nn_piped_grid.fit(feature_data, label_data)\n",
    "\n",
    "# your code goes here\n",
    "print(final_model.best_params_)\n",
    "print(final_model.best_score_)\n",
    "\n",
    "filename = 'finalized_model.sav'\n",
    "pickle.dump(final_model, open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q10. Now if someone wants to use your trained, saved classifier to classify a new record, they can load the saved model and just call predict on it. \n",
    "* Given this new record, classify it with your saved model and print out either \"Negative for disease\" or \"Positive for disease.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positive for disease\n"
     ]
    }
   ],
   "source": [
    "# some time later...\n",
    "\n",
    "# use this as the new record to classify\n",
    "record = [ 0.05905386, 0.2982129, 0.68613149, 0.75078865, 0.87119216, 0.88615694,\n",
    "  0.93600623, 0.98369184, -0.47426472, -0.57642756, -0.53115361, -0.42789774,\n",
    " -0.21907738, -0.20090532, -0.21496782, -0.2080998, 0.06692373, -2.81681183,\n",
    " -0.7117194 ]\n",
    "\n",
    " \n",
    "# load the model from disk\n",
    "loaded_model = pickle.load(open(filename, 'rb'))\n",
    "\n",
    "\n",
    "# run the model\n",
    "predic = loaded_model.predict([record])\n",
    "\n",
    "if predic == 1:\n",
    "    print('Positive for disease')\n",
    "else:\n",
    "    print('Negative for disease')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
